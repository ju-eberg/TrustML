
@article{verma_counterfactual_2022,
	title = {Counterfactual {Explanations} and {Algorithmic} {Recourses} for {Machine} {Learning}: {A} {Review}},
	volume = {56},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Counterfactual {Explanations} and {Algorithmic} {Recourses} for {Machine} {Learning}},
	url = {https://dl.acm.org/doi/10.1145/3677119},
	doi = {10.1145/3677119},
	abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine learning based systems. A burgeoning body of research seeks to define the goals and methods of
              explainability
              in machine learning. In this article, we seek to review and categorize research on
              counterfactual explanations
              , a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.},
	language = {en},
	number = {12},
	urldate = {2024-11-20},
	journal = {ACM Comput. Surv.},
	author = {Verma, Sahil and Boonsanong, Varich and Hoang, Minh and Hines, Keegan and Dickerson, John and Shah, Chirag},
	month = nov,
	year = {2022},
	keywords = {Overview},
	pages = {1--42},
	file = {Eingereichte Version:files/8/Verma et al. - 2024 - Counterfactual Explanations and Algorithmic Recourses for Machine Learning A Review.pdf:application/pdf},
}

@misc{bischl_counterfactuals_2023,
	address = {Munich},
	type = {Lecture},
	title = {Counterfactuals and {Adversarial} {Examples}},
	url = {https://slds-lmu.github.io/iml/chapters/07_counterfactuals_and_adversarial_examples/},
	language = {English},
	urldate = {2024-11-20},
	author = {Bischl, Bernd},
	year = {2023},
	keywords = {Overview},
	file = {PDF:files/39/Bischl - 2023 - Counterfactuals and Adversarial Examples.pdf:application/pdf},
}

@inproceedings{dandl_multi-objective_2020,
	address = {Cham},
	title = {Multi-{Objective} {Counterfactual} {Explanations}},
	isbn = {978-3-030-58112-1},
	doi = {10.1007/978-3-030-58112-1_31},
	abstract = {Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of ‘what-if scenarios’. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally difficult to balance a-priori. We propose the Multi-Objective Counterfactuals (MOC) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-offs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of MOC in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.},
	language = {en},
	booktitle = {Parallel {Problem} {Solving} from {Nature} – {PPSN} {XVI}},
	publisher = {Springer International Publishing},
	author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	editor = {Bäck, Thomas and Preuss, Mike and Deutz, André and Wang, Hao and Doerr, Carola and Emmerich, Michael and Trautmann, Heike},
	year = {2020},
	keywords = {Counterfactual explanations, Interpretability, Interpretable machine learning, Multi-objective optimization, NSGA-II, Maths},
	pages = {448--469},
	file = {Full Text PDF:files/14/Dandl et al. - 2020 - Multi-Objective Counterfactual Explanations.pdf:application/pdf},
}

@incollection{lewis_counterfactuals_1981,
	address = {Dordrecht},
	title = {Counterfactuals and {Comparative} {Possibility}},
	isbn = {978-94-009-9117-0},
	url = {https://doi.org/10.1007/978-94-009-9117-0_3},
	abstract = {In the last dozen years or so, our understanding of modality has been much improved by means of possible-world semantics: the project of analyzing modal language by systematically specifying the conditions under which a modal sentence is true at a possible world. I hope to do the same for counterfactual conditionals. I write A□→C for the counterfactual conditional with antecedent A and consequent C. It may be read as ‘If it were the case that A, then it would be the case that C’or some more idiomatic paraphrase thereof.},
	language = {en},
	urldate = {2024-12-04},
	booktitle = {{IFS}: {Conditionals}, {Belief}, {Decision}, {Chance} and {Time}},
	publisher = {Springer Netherlands},
	author = {Lewis, David},
	editor = {Harper, William L. and Stalnaker, Robert and Pearce, Glenn},
	year = {1981},
	doi = {10.1007/978-94-009-9117-0_3},
	keywords = {Philosophy},
	pages = {57--85},
	file = {PDF:files/15/Lewis - Counterfactuals and comparative possibility.pdf:application/pdf},
}

@inproceedings{karimi_model-agnostic_2020,
	title = {Model-{Agnostic} {Counterfactual} {Explanations} for {Consequential} {Decisions}},
	url = {https://proceedings.mlr.press/v108/karimi20a.html},
	abstract = {Predictive models are being increasingly used to support consequential decision making at the individual level in contexts such as pretrial bail and loan approval. As a result, there is increasing social and legal pressure to provide explanations that help the affected individuals not only to understand why a prediction was output, but also how to act to obtain a desired outcome. To this end, several works have proposed optimization-based methods to generate nearest counterfactual explanations. However, these methods are often restricted to a particular subset of models (e.g., decision trees or linear models) and differentiable distance functions. In contrast, we build on standard theory and tools from formal verification and propose a novel algorithm that solves a sequence of satisfiability problems, where both the distance function (objective) and predictive model (constraints) are represented as logic formulae. As shown by our experiments on real-world data, our algorithm is: i) model-agnostic (\{non-\}linear, \{non-\}differentiable, \{non-\}convex); ii) data-type-agnostic (heterogeneous features); iii) distance-agnostic (l0, l1, l8, and combinations thereof); iv) able to generate plausible and diverse counterfactuals for any sample (i.e., 100\% coverage); and v) at provably optimal distances.},
	language = {en},
	urldate = {2024-12-04},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Karimi, Amir-Hossein and Barthe, Gilles and Balle, Borja and Valera, Isabel},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {895--905},
	file = {Full Text PDF:files/19/Karimi et al. - 2020 - Model-Agnostic Counterfactual Explanations for Consequential Decisions.pdf:application/pdf;Supplementary PDF:files/20/Karimi et al. - 2020 - Model-Agnostic Counterfactual Explanations for Consequential Decisions.pdf:application/pdf},
}

@misc{grath_interpretable_2018,
	title = {Interpretable {Credit} {Application} {Predictions} {With} {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/1811.05245},
	doi = {10.48550/arXiv.1811.05245},
	abstract = {We predict credit applications with off-the-shelf, interchangeable black-box classifiers and we explain single predictions with counterfactual explanations. Counterfactual explanations expose the minimal changes required on the input data to obtain a different result e.g., approved vs rejected application. Despite their effectiveness, counterfactuals are mainly designed for changing an undesired outcome of a prediction i.e. loan rejected. Counterfactuals, however, can be difficult to interpret, especially when a high number of features are involved in the explanation. Our contribution is two-fold: i) we propose positive counterfactuals, i.e. we adapt counterfactual explanations to also explain accepted loan applications, and ii) we propose two weighting strategies to generate more interpretable counterfactuals. Experiments on the HELOC loan applications dataset show that our contribution outperforms the baseline counterfactual generation strategy, by leading to smaller and hence more interpretable counterfactuals.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Grath, Rory Mc and Costabello, Luca and Van, Chan Le and Sweeney, Paul and Kamiab, Farbod and Shen, Zhao and Lecue, Freddy},
	month = nov,
	year = {2018},
	note = {arXiv:1811.05245 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:files/22/Grath et al. - 2018 - Interpretable Credit Application Predictions With Counterfactual Explanations.pdf:application/pdf;Snapshot:files/23/1811.html:text/html},
}

@article{wachter_counterfactual_2017,
	title = {Counterfactual {Explanations} without {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	volume = {31},
	shorttitle = {Counterfactual {Explanations} without {Opening} the {Black} {Box}},
	url = {https://heinonline.org/HOL/Page?handle=hein.journals/hjlt31&id=859&div=&collection=},
	journal = {Harv. J. L. \& Tech.},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	year = {2017},
	keywords = {Maths, Optimization},
	pages = {841},
	file = {Counterfactual Explanations without Opening the Black Box\: Automated Decisions and the GDPR 31 Harvard Journal of Law & Technology (Harvard JOLT) 2017-2018:files/41/LandingPage.html:text/html;Preprint PDF:files/43/Wachter et al. - 2018 - Counterfactual Explanations without Opening the Black Box Automated Decisions and the GDPR.pdf:application/pdf},
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures} (with comments and a rejoinder by the author)},
	volume = {16},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Statistical {Modeling}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full},
	doi = {10.1214/ss/1009213726},
	abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
	number = {3},
	urldate = {2024-12-04},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	month = aug,
	year = {2001},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {199--231},
	file = {Full Text PDF:files/46/Breiman - 2001 - Statistical Modeling The Two Cultures (with comments and a rejoinder by the author).pdf:application/pdf},
}

@misc{dandl_counterfactuals_2023,
	title = {counterfactuals: {An} {R} {Package} for {Counterfactual} {Explanation} {Methods}},
	shorttitle = {counterfactuals},
	url = {http://arxiv.org/abs/2304.06569},
	doi = {10.48550/arXiv.2304.06569},
	abstract = {Counterfactual explanation methods provide information on how feature values of individual observations must be changed to obtain a desired prediction. Despite the increasing amount of proposed methods in research, only a few implementations exist whose interfaces and requirements vary widely. In this work, we introduce the counterfactuals R package, which provides a modular and unified R6-based interface for counterfactual explanation methods. We implemented three existing counterfactual explanation methods and propose some optional methodological extensions to generalize these methods to different scenarios and to make them more comparable. We explain the structure and workflow of the package using real use cases and show how to integrate additional counterfactual explanation methods into the package. In addition, we compared the implemented methods for a variety of models and datasets with regard to the quality of their counterfactual explanations and their runtime behavior.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Dandl, Susanne and Hofheinz, Andreas and Binder, Martin and Bischl, Bernd and Casalicchio, Giuseppe},
	month = sep,
	year = {2023},
	note = {arXiv:2304.06569 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	annote = {Comment: 49 pages LaTeX, updated benchmark results},
	file = {Preprint PDF:files/53/Dandl et al. - 2023 - counterfactuals An R Package for Counterfactual Explanation Methods.pdf:application/pdf;Snapshot:files/54/2304.html:text/html},
}

@inproceedings{mothilal_explaining_2020,
	address = {Barcelona Spain},
	title = {Explaining machine learning classifiers through diverse counterfactual explanations},
	isbn = {978-1-4503-6936-7},
	url = {https://dl.acm.org/doi/10.1145/3351095.3372850},
	doi = {10.1145/3351095.3372850},
	language = {en},
	urldate = {2024-12-04},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
	month = jan,
	year = {2020},
	pages = {607--617},
	file = {Volltext:files/56/Mothilal et al. - 2020 - Explaining machine learning classifiers through diverse counterfactual explanations.pdf:application/pdf},
}

@misc{chou_counterfactuals_2021,
	title = {Counterfactuals and {Causability} in {Explainable} {Artificial} {Intelligence}: {Theory}, {Algorithms}, and {Applications}},
	shorttitle = {Counterfactuals and {Causability} in {Explainable} {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2103.04244},
	doi = {10.48550/arXiv.2103.04244},
	abstract = {There has been a growing interest in model-agnostic methods that can make deep learning models more transparent and explainable to a user. Some researchers recently argued that for a machine to achieve a certain degree of human-level explainability, this machine needs to provide human causally understandable explanations, also known as causability. A specific class of algorithms that have the potential to provide causability are counterfactuals. This paper presents an in-depth systematic review of the diverse existing body of literature on counterfactuals and causability for explainable artificial intelligence. We performed an LDA topic modelling analysis under a PRISMA framework to find the most relevant literature articles. This analysis resulted in a novel taxonomy that considers the grounding theories of the surveyed algorithms, together with their underlying properties and applications in real-world data. This research suggests that current model-agnostic counterfactual algorithms for explainable AI are not grounded on a causal theoretical formalism and, consequently, cannot promote causability to a human decision-maker. Our findings suggest that the explanations derived from major algorithms in the literature provide spurious correlations rather than cause/effects relationships, leading to sub-optimal, erroneous or even biased explanations. This paper also advances the literature with new directions and challenges on promoting causability in model-agnostic approaches for explainable artificial intelligence.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Chou, Yu-Liang and Moreira, Catarina and Bruza, Peter and Ouyang, Chun and Jorge, Joaquim},
	month = jun,
	year = {2021},
	note = {arXiv:2103.04244 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:files/58/Chou et al. - 2021 - Counterfactuals and Causability in Explainable Artificial Intelligence Theory, Algorithms, and Appl.pdf:application/pdf;Snapshot:files/59/2103.html:text/html},
}
